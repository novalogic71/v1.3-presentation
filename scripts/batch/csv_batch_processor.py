#!/usr/bin/env python3
"""
CSV-Based Batch Sync Processor with Automated Reporting, Auto-Repair, and Packaging

Processes file pairs from CSV input and generates comprehensive reports.
Optionally performs intelligent auto-repair and creates repair packages per row.
"""

import os
import sys
import csv
import json
import time
import argparse
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed

def process_csv_row(row_data):
    """Process a single CSV row - designed for multiprocessing"""
    (
        master_file,
        dub_file,
        episode_name,
        output_dir,
        chunk_size,
        generate_plot,
        auto_repair,
        repair_threshold,
        repair_output_dir,
        create_package,
        package_dir,
        use_optimized_cli,
        gpu_enabled,
        max_chunks,
    ) = row_data
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate safe filename from episode name
    safe_name = "".join(c for c in episode_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_name = safe_name.replace(' ', '_')
    
    json_output = output_dir / f"{safe_name}_analysis.json"
    plot_output = output_dir / f"{safe_name}_sync_chart.png" if generate_plot else None
    report_output = output_dir / f"{safe_name}_formatted_report.md"
    repaired_output: Optional[Path] = None
    package_result: Optional[Dict[str, Any]] = None
    
    print(f"🚀 Processing: {episode_name}")
    start_time = time.time()
    
    try:
        # Step 1: Run sync analysis (choose engine)
        if use_optimized_cli:
            # Use optimized GPU-capable CLI
            cmd = [
                'python', '-m', 'sync_analyzer.cli.optimized_sync_cli',
                master_file, dub_file,
                '--chunk-size', str(chunk_size),
                '--output-dir', str(output_dir),
            ]
            # Prefer JSON-only unless plots explicitly requested
            if not generate_plot:
                cmd.append('--json-only')
            if isinstance(max_chunks, int) and max_chunks > 0:
                cmd.extend(['--max-chunks', str(max_chunks)])
            if gpu_enabled:
                cmd.append('--gpu')
            # Skip visualization for speed; plot will be generated by continuous variant only
            if not generate_plot:
                cmd.append('--no-visualization')
            # Be quiet to keep batch logs tidy
            cmd.append('--quiet')
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
            # Try to resolve JSON path produced by optimized CLI
            json_output_path = None
            try:
                # Look for new filename pattern (with both old and new patterns for compatibility)
                candidates = list(Path(output_dir).glob(f"sync_report_*.json")) + \
                           list(Path(output_dir).glob(f"optimized_sync_report_*.json"))
                if candidates:
                    # Pick the most recent candidate
                    json_output_path = max(candidates, key=lambda p: p.stat().st_mtime)
                    json_output = json_output_path
            except Exception:
                pass
        else:
            # Legacy continuous monitor
            cmd = [
                'python', '-m', 'scripts.monitoring.continuous_sync_monitor',
                master_file, dub_file,
                '--output', str(json_output),
                '--chunk-size', str(chunk_size),
                '--quiet'
            ]
            
            if plot_output:
                cmd.extend(['--plot', str(plot_output)])
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
        
        # Determine acceptable return codes per engine
        # Optimized CLI exits: 0=success, 1=warning(Fair), 2=poor quality; all mean analysis completed
        # Continuous monitor exits: 0=success, 2=drift detected (non-fatal)
        allowed_rc = {0, 1, 2} if use_optimized_cli else {0, 2}
        if result.returncode not in allowed_rc:  # non-acceptable exit code for this engine
            return {
                'episode': episode_name,
                'status': 'ANALYSIS_FAILED',
                'error': result.stderr,
                'duration': time.time() - start_time
            }
        
        # Step 2: Generate formatted report
        if Path(json_output).exists():
            report_cmd = [
                'python', '-m', 'scripts.repair.sync_report_analyzer',
                str(json_output),
                '--name', episode_name,
                '--output', str(report_output)
            ]
            
            report_result = subprocess.run(report_cmd, capture_output=True, text=True)
            
            if report_result.returncode != 0:
                print(f"⚠️  Report generation failed for {episode_name}")
        
        # Optional: Auto-repair + package
        analysis_data = None
        if auto_repair and Path(json_output).exists():
            try:
                with open(json_output, 'r') as f:
                    analysis_data = json.load(f)
                # Determine offset in ms
                offset_ms = abs(analysis_data.get('offset_seconds', 0) * 1000)
                if offset_ms >= repair_threshold:
                    # Compute repaired output path
                    dub_suffix = Path(dub_file).suffix or '.mov'
                    ro_dir = Path(repair_output_dir)
                    ro_dir.mkdir(parents=True, exist_ok=True)
                    repaired_output = ro_dir / f"{safe_name}_REPAIRED{dub_suffix}"
                    
                    # Prepare temp analysis file for repairer
                    import tempfile
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tf:
                        json.dump(analysis_data, tf, indent=2, default=str)
                        temp_analysis_file = tf.name
                    try:
                        # Perform intelligent repair
                        import sys
                        import os
                        sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
                        from scripts.repair.intelligent_sync_repair import IntelligentSyncRepairer
                        repairer = IntelligentSyncRepairer()
                        repair_result = repairer.repair_file(dub_file, temp_analysis_file, str(repaired_output))
                        if not repair_result.get('success'):
                            # Mark failure but still return other outputs
                            repaired_output = None
                        else:
                            # Optional package creation
                            if create_package:
                                try:
                                    from sync_repair_packager import SyncRepairPackager
                                    pkg_dir = str(package_dir)
                                    packager = SyncRepairPackager(pkg_dir)
                                    package_result = packager.create_repair_package(
                                        original_file=dub_file,
                                        analysis_data=analysis_data,
                                        repaired_file=str(repaired_output),
                                        episode_name=episode_name,
                                        include_visualization=True,
                                        create_zip=True,
                                    )
                                except Exception as e:
                                    # Packaging failed; record error later
                                    package_result = {"success": False, "error": str(e)}
                    finally:
                        try:
                            os.unlink(temp_analysis_file)
                        except Exception:
                            pass
            except Exception as e:
                # Ignore auto-repair failure and proceed with analysis outputs only
                pass

        duration = time.time() - start_time

        # Determine status from analysis result / engine
        if use_optimized_cli:
            # Treat all completed analyses as success; quality is reflected in reports
            status = "SUCCESS"
        else:
            # Continuous monitor: rc 2 used for drift detected (non-fatal)
            status = "DRIFT_DETECTED" if result.returncode == 2 else "SUCCESS"

        out: Dict[str, Any] = {
            'episode': episode_name,
            'master_file': master_file,
            'dub_file': dub_file,
            'status': status,
            'duration': duration,
            'json_output': str(json_output),
            'plot_output': str(plot_output) if plot_output else None,
            'report_output': str(report_output),
            'return_code': result.returncode,
        }
        if repaired_output is not None and Path(repaired_output).exists():
            out['repaired_output'] = str(repaired_output)
        if package_result is not None:
            out['package'] = package_result
        return out
        
    except subprocess.TimeoutExpired:
        return {
            'episode': episode_name,
            'status': 'TIMEOUT',
            'duration': time.time() - start_time,
            'error': 'Analysis timed out after 1 hour'
        }
    except Exception as e:
        return {
            'episode': episode_name,
            'status': 'ERROR',
            'duration': time.time() - start_time,
            'error': str(e)
        }

def read_csv_file(csv_file: str) -> List[Dict[str, str]]:
    """Read and validate CSV file"""
    rows = []
    
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        
        # Validate required columns
        required_columns = ['master_file', 'dub_file', 'episode_name']
        missing_columns = [col for col in required_columns if col not in reader.fieldnames]
        
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        for row_num, row in enumerate(reader, start=2):
            # Validate file paths exist
            if not os.path.exists(row['master_file']):
                print(f"⚠️  Row {row_num}: Master file not found: {row['master_file']}")
                continue
            
            if not os.path.exists(row['dub_file']):
                print(f"⚠️  Row {row_num}: Dub file not found: {row['dub_file']}")
                continue
            
            rows.append(row)
    
    return rows

def main():
    parser = argparse.ArgumentParser(
        description="CSV-Based Batch Sync Processor (analyze + optional auto-repair + package)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
CSV Format Requirements:
  Required columns:
    - master_file: Path to master/reference video file
    - dub_file: Path to dub/test video file  
    - episode_name: Display name for episode/file
  
  Optional columns:
    - chunk_size: Analysis chunk size (default: 45)
    - notes: Any additional notes

Example CSV:
  master_file,dub_file,episode_name,chunk_size
  /path/master1.mov,/path/dub1.mov,Episode 101,45
  /path/master2.mov,/path/dub2.mov,Episode 102,30

Examples:
  # Analyze only
  %(prog)s files.csv --output-dir results/

  # Analyze + auto-repair + package in one shot
  %(prog)s batch.csv --output-dir results/ --auto-repair --repair-threshold 100 \
      --repair-output-dir ./repaired_sync_files --create-package --package-dir ./repair_packages \
      --plot --max-workers 3
        """
    )
    
    parser.add_argument('csv_file', help='CSV file containing file pairs and metadata')
    parser.add_argument('--output-dir', required=True, help='Output directory for all results')
    parser.add_argument('--chunk-size', type=float, default=45.0, 
                       help='Default chunk size in seconds (can be overridden per row)')
    parser.add_argument('--max-workers', type=int, 
                       help='Max parallel processes (default: GPU count)')
    parser.add_argument('--plot', action='store_true', 
                       help='Generate sync visualization plots')
    parser.add_argument('--skip-reports', action='store_true',
                       help='Skip formatted report generation (JSON only)')
    
    # Auto-repair + packaging options
    parser.add_argument('--auto-repair', action='store_true',
                       help='Automatically repair sync issues if detected (per row)')
    parser.add_argument('--repair-threshold', type=float, default=100.0,
                       help='Offset threshold in ms to trigger auto-repair (default: 100)')
    parser.add_argument('--repair-output-dir', default='./repaired_sync_files',
                       help='Directory to write repaired files (default: ./repaired_sync_files)')
    parser.add_argument('--create-package', action='store_true',
                       help='Create comprehensive repair package per row when repaired')
    parser.add_argument('--package-dir', default='./repair_packages',
                       help='Directory for repair packages (default: ./repair_packages)')

    # Engine selection
    parser.add_argument('--use-optimized-cli', action='store_true', default=True,
                       help='Use GPU-accelerated optimized CLI for analysis (default: True)')
    parser.add_argument('--use-legacy-monitor', action='store_true',
                       help='Use legacy continuous_sync_monitor instead of optimized CLI')
    parser.add_argument('--gpu', action='store_true',
                       help='Enable GPU acceleration (with optimized CLI)')
    parser.add_argument('--max-chunks', type=int,
                       help='Max chunks for optimized CLI (default per tool)')
    
    args = parser.parse_args()

    # Override optimized CLI if legacy monitor explicitly requested
    if args.use_legacy_monitor:
        args.use_optimized_cli = False

    if not os.path.exists(args.csv_file):
        print(f"Error: CSV file not found: {args.csv_file}")
        sys.exit(1)
    
    # Read and validate CSV
    try:
        csv_rows = read_csv_file(args.csv_file)
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        sys.exit(1)
    
    if not csv_rows:
        print("No valid file pairs found in CSV")
        sys.exit(1)
    
    # Setup output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Determine worker count
    try:
        result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)
        gpu_count = len([line for line in result.stdout.split('\n') if 'GPU' in line])
    except:
        gpu_count = 1
        
    max_workers = args.max_workers or gpu_count or 1
    
    print(f"🔥 CSV Batch Sync Processor")
    print(f"📁 CSV file: {args.csv_file}")
    print(f"📊 File pairs to process: {len(csv_rows)}")
    print(f"🎯 GPUs available: {gpu_count}")
    print(f"⚡ Max parallel workers: {max_workers}")
    print(f"📈 Generate plots: {'Yes' if args.plot else 'No'}")
    if args.auto_repair:
        print(f"🛠️  Auto-repair: Enabled (threshold: {args.repair_threshold:.0f}ms)")
        print(f"📦 Create package: {'Yes' if args.create_package else 'No'}")
        print(f"🗂️  Repaired dir: {args.repair_output_dir}")
        if args.create_package:
            print(f"📦 Package dir: {args.package_dir}")
    if args.use_optimized_cli:
        print(f"🚀 Engine: optimized CLI ({'GPU ON' if args.gpu else 'GPU OFF'})")
        if args.max_chunks:
            print(f"🔢 Max chunks: {args.max_chunks}")
    print(f"💾 Output directory: {output_dir}")
    print("-" * 60)
    
    # Prepare processing arguments
    process_args = []
    for row in csv_rows:
        chunk_size = float(row.get('chunk_size', args.chunk_size))
        process_args.append((
            row['master_file'],
            row['dub_file'], 
            row['episode_name'],
            args.output_dir,
            chunk_size,
            args.plot,
            args.auto_repair,
            args.repair_threshold,
            args.repair_output_dir,
            args.create_package,
            args.package_dir,
            args.use_optimized_cli,
            args.gpu,
            args.max_chunks,
        ))
    
    # Process all rows
    start_time = time.time()
    results = []
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_episode = {executor.submit(process_csv_row, args): args[2] 
                           for args in process_args}
        
        for future in as_completed(future_to_episode):
            result = future.result()
            results.append(result)
            
            status_icon = {
                'SUCCESS': '✅',
                'DRIFT_DETECTED': '⚠️',
                'ANALYSIS_FAILED': '❌',
                'TIMEOUT': '⏰',
                'ERROR': '💥'
            }.get(result['status'], '❓')
            
            print(f"{status_icon} {result['episode']} ({result['duration']:.1f}s)")
    
    total_time = time.time() - start_time
    
    # Generate summary report
    print("\n" + "="*60)
    print("CSV BATCH PROCESSING SUMMARY")
    print("="*60)
    
    success_count = sum(1 for r in results if r['status'] in ['SUCCESS', 'DRIFT_DETECTED'])
    failed_count = len(results) - success_count
    drift_count = sum(1 for r in results if r['status'] == 'DRIFT_DETECTED')
    repairs_count = sum(1 for r in results if r.get('repaired_output'))
    packages_count = sum(1 for r in results if isinstance(r.get('package'), dict) and r['package'].get('success'))
    
    print(f"Total episodes processed: {len(results)}")
    print(f"Successful analyses: {success_count}")
    print(f"Episodes with drift detected: {drift_count}")
    print(f"Failed analyses: {failed_count}")
    print(f"Total processing time: {total_time/60:.1f} minutes")
    print(f"Average time per episode: {total_time/len(results):.1f} seconds")
    print(f"Throughput: {len(results)/(total_time/60):.1f} episodes/minute")
    if args.auto_repair:
        print(f"Repairs performed: {repairs_count}")
        if args.create_package:
            print(f"Packages created: {packages_count}")
    
    # Export comprehensive summary
    summary_data = {
        'processing_summary': {
            'csv_file': args.csv_file,
            'total_episodes': len(results),
            'successful': success_count,
            'drift_detected': drift_count,
            'failed': failed_count,
            'total_time_seconds': total_time,
            'average_time_per_episode': total_time/len(results),
            'throughput_episodes_per_minute': len(results)/(total_time/60),
            'gpu_count': gpu_count,
            'max_workers': max_workers
        },
        'episode_results': results
    }
    
    summary_file = output_dir / "batch_processing_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(summary_data, f, indent=2)
    
    # Generate CSV summary for easy viewing
    csv_summary_file = output_dir / "batch_results_summary.csv"
    with open(csv_summary_file, 'w', newline='') as f:
        writer = csv.writer(f)
        headers = ['Episode', 'Status', 'Duration_Seconds', 'JSON_Output', 'Report_Output']
        if args.auto_repair:
            headers.extend(['Repaired_Output'])
        if args.auto_repair and args.create_package:
            headers.extend(['Package_Dir', 'Package_Zip'])
        writer.writerow(headers)
        
        for result in results:
            row = [
                result['episode'],
                result['status'],
                f"{result['duration']:.1f}",
                result.get('json_output', ''),
                result.get('report_output', ''),
            ]
            if args.auto_repair:
                row.append(result.get('repaired_output', ''))
            if args.auto_repair and args.create_package:
                pkg = result.get('package') or {}
                row.append(pkg.get('package_directory', ''))
                row.append(pkg.get('zip_file', ''))
            writer.writerow(row)
    
    print(f"\n📋 Summary exported to: {summary_file}")
    print(f"📊 CSV summary: {csv_summary_file}")
    
    if failed_count == 0:
        print(f"\n🎉 All episodes processed successfully!")
        if drift_count > 0:
            print(f"⚠️  {drift_count} episodes have sync drift - check individual reports")
        sys.exit(0)
    else:
        print(f"\n⚠️  {failed_count} episodes failed - check logs for details")
        sys.exit(1)

if __name__ == "__main__":
    main()
